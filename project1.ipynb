{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run proj1_helpers.py\n",
    "DATA_TRAIN_PATH = 'train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1. -1. ...  1. -1. -1.]\n",
      "----------------------------------\n",
      "[[ 138.47    51.655   97.827 ...    1.24    -2.475  113.497]\n",
      " [ 160.937   68.768  103.235 ... -999.    -999.      46.226]\n",
      " [-999.     162.172  125.953 ... -999.    -999.      44.251]\n",
      " ...\n",
      " [ 105.457   60.526   75.839 ... -999.    -999.      41.992]\n",
      " [  94.951   19.362   68.812 ... -999.    -999.       0.   ]\n",
      " [-999.      72.756   70.831 ... -999.    -999.       0.   ]]\n",
      "----------------------------------\n",
      "[100000 100001 100002 ... 349997 349998 349999]\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print('----------------------------------')\n",
    "print(tX)\n",
    "print('----------------------------------')\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = np.insert(tX, 0, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.   ,  138.47 ,   51.655, ...,    1.24 ,   -2.475,  113.497],\n",
       "       [   1.   ,  160.937,   68.768, ..., -999.   , -999.   ,   46.226],\n",
       "       [   1.   , -999.   ,  162.172, ..., -999.   , -999.   ,   44.251],\n",
       "       ...,\n",
       "       [   1.   ,  105.457,   60.526, ..., -999.   , -999.   ,   41.992],\n",
       "       [   1.   ,   94.951,   19.362, ..., -999.   , -999.   ,    0.   ],\n",
       "       [   1.   , -999.   ,   72.756, ..., -999.   , -999.   ,    0.   ]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "There seems to be quiet a few incorrect values \"-999\", for now we'll set these equal to the average of their respective column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = np.where(tx==-999, np.nan, tx) # replace -999 value with nan\n",
    "col_mean = np.nanmean(tx, axis=0)\n",
    "inds_nan = np.where(np.isnan(tx))\n",
    "tx[inds_nan] = np.take(col_mean, inds_nan[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  1.38470000e+02,  5.16550000e+01, ...,\n",
       "         1.24000000e+00, -2.47500000e+00,  1.13497000e+02],\n",
       "       [ 1.00000000e+00,  1.60937000e+02,  6.87680000e+01, ...,\n",
       "        -1.18452642e-02, -1.58228913e-03,  4.62260000e+01],\n",
       "       [ 1.00000000e+00,  1.21858528e+02,  1.62172000e+02, ...,\n",
       "        -1.18452642e-02, -1.58228913e-03,  4.42510000e+01],\n",
       "       ...,\n",
       "       [ 1.00000000e+00,  1.05457000e+02,  6.05260000e+01, ...,\n",
       "        -1.18452642e-02, -1.58228913e-03,  4.19920000e+01],\n",
       "       [ 1.00000000e+00,  9.49510000e+01,  1.93620000e+01, ...,\n",
       "        -1.18452642e-02, -1.58228913e-03,  0.00000000e+00],\n",
       "       [ 1.00000000e+00,  1.21858528e+02,  7.27560000e+01, ...,\n",
       "        -1.18452642e-02, -1.58228913e-03,  0.00000000e+00]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing predictions {-1;1} --> {0;1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log = np.copy(y)\n",
    "y_log = np.where(y_log==-1, 0, y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 1., 0., 0.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the min and max values for each columns of tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "minmax = dataset_minmax(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 1.0],\n",
       " [9.044, 1192.026],\n",
       " [0.0, 690.075],\n",
       " [6.329, 1349.351],\n",
       " [0.0, 2834.999],\n",
       " [0.0, 8.503],\n",
       " [13.602, 4974.979],\n",
       " [-18.066, 16.69],\n",
       " [0.208, 5.684],\n",
       " [0.0, 2834.999],\n",
       " [46.104, 1852.462],\n",
       " [0.047, 19.773],\n",
       " [-1.414, 1.414],\n",
       " [0.0, 1.0],\n",
       " [20.0, 764.408],\n",
       " [-2.499, 2.497],\n",
       " [-3.142, 3.142],\n",
       " [26.0, 560.271],\n",
       " [-2.505, 2.503],\n",
       " [-3.142, 3.142],\n",
       " [0.109, 2842.617],\n",
       " [-3.142, 3.142],\n",
       " [13.678, 2003.976],\n",
       " [0.0, 3.0],\n",
       " [30.0, 1120.573],\n",
       " [-4.499, 4.499],\n",
       " [-3.142, 3.141],\n",
       " [30.0, 721.456],\n",
       " [-4.5, 4.5],\n",
       " [-3.142, 3.142],\n",
       " [0.0, 1633.433]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_normalized = np.copy(tx)\n",
    "normalize_dataset(tx_normalized, minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.10940657 0.07485418 ... 0.63777778 0.10614258 0.06948372]\n",
      " [1.         0.1283984  0.09965294 ... 0.49868386 0.4997482  0.02829991]\n",
      " [1.         0.09536454 0.23500634 ... 0.49868386 0.4997482  0.0270908 ]\n",
      " ...\n",
      " [1.         0.08149997 0.08770931 ... 0.49868386 0.4997482  0.02570782]\n",
      " [1.         0.07261903 0.02805782 ... 0.49868386 0.4997482  0.        ]\n",
      " [1.         0.09536454 0.10543202 ... 0.49868386 0.4997482  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tx_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardized dataset (works well if features are distributed according to a gaussian curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_standardized = np.copy(tx)\n",
    "means = column_means(tx_standardized)\n",
    "stdevs = column_stdevs(tx_standardized, means)\n",
    "standardize_dataset(tx_standardized, means, stdevs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00  3.14910026e-01  6.83318303e-02 ...  1.14381645e+00\n",
      "  -2.52713783e+00  4.12509672e-01]\n",
      " [ 1.00000000e+00  7.40825545e-01  5.52503718e-01 ...  2.27720450e-14\n",
      "  -9.07313667e-15 -2.73819417e-01]\n",
      " [ 1.00000000e+00 -1.00190088e-12  3.19514914e+00 ...  2.27720450e-14\n",
      "  -9.07313667e-15 -2.93969258e-01]\n",
      " ...\n",
      " [ 1.00000000e+00 -3.10930051e-01  3.19315808e-01 ...  2.27720450e-14\n",
      "  -9.07313667e-15 -3.17016595e-01]\n",
      " [ 1.00000000e+00 -5.10096314e-01 -8.45322280e-01 ...  2.27720450e-14\n",
      "  -9.07313667e-15 -7.45437922e-01]\n",
      " [ 1.00000000e+00 -1.00190088e-12  6.65334753e-01 ...  2.27720450e-14\n",
      "  -9.07313667e-15 -7.45437922e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(tx_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8 # 100ratio% of the data will be used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = split_data(tx, y, ratio, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_log, y_train_log, x_test_log, y_test_log = split_data(tx, y_log, ratio, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_log_norm, y_train_log_norm, x_test_log_norm, y_test_log_norm = split_data(tx_normalized, y_log, ratio, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_log_stand, y_train_log_stand, x_test_log_stand, y_test_log_stand = split_data(tx_standardized, y_log, ratio, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical accuracy train :  0.746285  || f1_score train:  0.2892969865966419\n",
      "Categorical accuracy test :  0.747645431826469  || f1_score test:  0.28738319536360796\n"
     ]
    }
   ],
   "source": [
    "weights_LS = least_squares(y_train, x_train)[1]\n",
    "cat_accuracy_train, f1_score_train = metrics(weights_LS,y_train,x_train)\n",
    "cat_accuracy_test, f1_score_test = metrics(weights_LS,y_test,x_test)\n",
    "\n",
    "\n",
    "print(\"Categorical accuracy train : \",cat_accuracy_train,\" || f1_score train: \",f1_score_train)\n",
    "print(\"Categorical accuracy test : \",cat_accuracy_test,\" || f1_score test: \",f1_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical accuracy train :  0.693025  || f1_score train:  0.10416656792534208\n",
      "Categorical accuracy test :  0.693083483782606  || f1_score test:  0.10352591942924126\n"
     ]
    }
   ],
   "source": [
    "weights_LSGD1 = least_squares_GD(y_train, x_train, np.zeros(31), 100, 0.000001)[1]\n",
    "cat_accuracy_train, f1_score_train = metrics(weights_LSGD1,y_train,x_train)\n",
    "cat_accuracy_test, f1_score_test = metrics(weights_LSGD1,y_test,x_test)\n",
    "\n",
    "\n",
    "print(\"Categorical accuracy train : \",cat_accuracy_train,\" || f1_score train: \",f1_score_train)\n",
    "print(\"Categorical accuracy test : \",cat_accuracy_test,\" || f1_score test: \",f1_score_test)\n",
    "\n",
    "# THIS CONVERGES INTO A LOCAL MINIMUM, solution: Dynamic Step Size ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical accuracy train :  0.73289  || f1_score train:  0.2737637991006309\n",
      "Categorical accuracy test :  0.7327931982106172  || f1_score test:  0.2720790496826641\n"
     ]
    }
   ],
   "source": [
    "weights_LSGD2 = least_squares_GD(y, tx, [-1.15430619e+00,  1.82637143e-04, -7.20672107e-03, -6.45384640e-03,\n",
    "        -1.73123839e-05,  2.32665195e-02,  4.20381986e-04,  2.50304362e-03,\n",
    "         3.60203545e-01, -1.26385429e-03, -2.84568774e+00, -2.22719927e-01,\n",
    "         9.89163555e-02,  3.56752661e-01,  2.85396180e+00, -6.42020723e-04,\n",
    "        -4.57219107e-04,  2.85879539e+00, -6.80776737e-04,  1.38605239e-03,\n",
    "         3.15125355e-03,  5.15272243e-04, -3.71558734e-04,  4.27220740e-02,\n",
    "        -1.01225645e-03,  4.70620275e-04,  1.34341503e-04, -2.12423006e-03,\n",
    "         1.42389540e-03, -1.78105047e-03,  2.84577828e+00], 100, 0.000001)[1]\n",
    "\n",
    "cat_accuracy_train, f1_score_train = metrics(weights_LSGD2,y_train,x_train)\n",
    "cat_accuracy_test, f1_score_test = metrics(weights_LSGD2,y_test,x_test)\n",
    "\n",
    "\n",
    "print(\"Categorical accuracy train : \",cat_accuracy_train,\" || f1_score train: \",f1_score_train)\n",
    "print(\"Categorical accuracy test : \",cat_accuracy_test,\" || f1_score test: \",f1_score_test)\n",
    "\n",
    "# IT STAYS IN THE ABSOLUTE MINIMUM WHEN USING THAT AS A STARTING POINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical accuracy train :  0.721595  || f1_score train:  0.375502578889656\n",
      "Categorical accuracy test :  0.7212315792282175  || f1_score train:  0.37321411778322733\n"
     ]
    }
   ],
   "source": [
    "weights_LSSGD = least_squares_SGD(y, tx, np.zeros(31), 100, 0.000001)[1]\n",
    "\n",
    "cat_accuracy_train, f1_score_train = metrics(weights_LSSGD,y_train,x_train)\n",
    "cat_accuracy_test, f1_score_test = metrics(weights_LSSGD,y_test,x_test)\n",
    "\n",
    "\n",
    "print(\"Categorical accuracy train : \",cat_accuracy_train,\" || f1_score train: \",f1_score_train)\n",
    "print(\"Categorical accuracy test : \",cat_accuracy_test,\" || f1_score train: \",f1_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.747645431826469 0.28738319536360796\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.005\n",
    "\n",
    "weights_RR = ridge_regression(y_train, x_train, lambda_)[1]\n",
    "cat_acc_test, f1_score_test = metrics(weights_RR, y_test, x_test)\n",
    "\n",
    "print(cat_acc_test, f1_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jetma\\Documents\\GitHub\\ML\\implementations.py:92: RuntimeWarning: divide by zero encountered in log\n",
      "  def compute_log_gradient(tx, y, ws, lr):\n",
      "C:\\Users\\jetma\\Documents\\GitHub\\ML\\implementations.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  def compute_log_gradient(tx, y, ws, lr):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0  loss:  nan  accuracy:  0.65668  f1_score:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jetma\\Documents\\GitHub\\ML\\implementations.py:93: RuntimeWarning: divide by zero encountered in log\n",
      "  N = y.shape[0]\n",
      "C:\\Users\\jetma\\Documents\\GitHub\\ML\\implementations.py:93: RuntimeWarning: invalid value encountered in multiply\n",
      "  N = y.shape[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  100  loss:  nan  accuracy:  0.65698  f1_score:  0.0005998200539838049\n",
      "Step:  200  loss:  nan  accuracy:  0.71535  f1_score:  0.37105693970467263\n",
      "Step:  300  loss:  nan  accuracy:  0.578615  f1_score:  0.47623111002754215\n",
      "Step:  400  loss:  nan  accuracy:  0.59427  f1_score:  0.47406870505655485\n",
      "Step:  500  loss:  nan  accuracy:  0.600145  f1_score:  0.4731017792181518\n",
      "Step:  600  loss:  nan  accuracy:  0.603315  f1_score:  0.4723495264283532\n",
      "Step:  700  loss:  nan  accuracy:  0.60513  f1_score:  0.4717892291704872\n",
      "Step:  800  loss:  nan  accuracy:  0.60655  f1_score:  0.4715089263878699\n",
      "Step:  900  loss:  nan  accuracy:  0.60778  f1_score:  0.47129278947026476\n",
      "Step:  1000  loss:  nan  accuracy:  0.608445  f1_score:  0.47099477080211616\n",
      "Step:  1100  loss:  nan  accuracy:  0.6091  f1_score:  0.4707375632732333\n",
      "Step:  1200  loss:  nan  accuracy:  0.60997  f1_score:  0.47069663591492483\n",
      "Step:  1300  loss:  nan  accuracy:  0.610485  f1_score:  0.4706557063658956\n",
      "Step:  1400  loss:  nan  accuracy:  0.610865  f1_score:  0.4705153598494987\n",
      "Step:  1500  loss:  nan  accuracy:  0.61117  f1_score:  0.47026384124398146\n",
      "Step:  1600  loss:  nan  accuracy:  0.61149  f1_score:  0.4700590557204492\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-ae99438546d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'run'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'implementations.py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mweights_log_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_log\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_log\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\ML\\implementations.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[1;34m(tx, y, ws, lr, num_iterations)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mws\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mws\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcompute_log_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcost_log_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\ML\\implementations.py\u001b[0m in \u001b[0;36mcompute_log_gradient\u001b[1;34m(tx, y, ws, lr)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_log_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[0msigmoid_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msigmoid_prediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\ML\\implementations.py\u001b[0m in \u001b[0;36msigmoid_activation\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msigmoid_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iterations = 10000\n",
    "lr = 0.005\n",
    "w = np.zeros(31)\n",
    "%run implementations.py\n",
    "weights_log_reg = logistic_regression(x_train_log, y_train_log, w, lr, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0  loss:  0.6874130944712561  accuracy_train:  0.70014  f1_score_train:  0.21663434316413652  accuracy_test:  0.699122206312644  f1_score_test:  0.2138897272568424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jetma\\Documents\\GitHub\\ML\\implementations.py:93: RuntimeWarning: divide by zero encountered in log\n",
      "  * np.log(1-sigmoid_prediction))\n",
      "C:\\Users\\jetma\\Documents\\GitHub\\ML\\implementations.py:93: RuntimeWarning: invalid value encountered in multiply\n",
      "  * np.log(1-sigmoid_prediction))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  10000  loss:  nan  accuracy_train:  0.713885  f1_score_train:  0.1746911805641117  accuracy_test:  0.7124358552485304  f1_score_test:  0.17108141738303012\n",
      "Step:  20000  loss:  nan  accuracy_train:  0.71391  f1_score_train:  0.17559863169897377  accuracy_test:  0.7126759811812418  f1_score_test:  0.1722408706324114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jetma\\Documents\\GitHub\\ML\\implementations.py:92: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.mean(y*np.log(sigmoid_prediction)+(1-y)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:75: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  30000  loss:  nan  accuracy_train:  0.71392  f1_score_train:  0.17576503853696357  accuracy_test:  0.712569258544481  f1_score_test:  0.1722854356306892\n",
      "Step:  40000  loss:  nan  accuracy_train:  0.71396  f1_score_train:  0.17587318670393967  accuracy_test:  0.7126404069689882  f1_score_test:  0.17238941217583525\n",
      "Step:  50000  loss:  nan  accuracy_train:  0.713985  f1_score_train:  0.17596468667633405  accuracy_test:  0.7126848747343051  f1_score_test:  0.17244882203314074\n",
      "Step:  60000  loss:  nan  accuracy_train:  0.713945  f1_score_train:  0.1759314150212048  accuracy_test:  0.7126670876281783  f1_score_test:  0.17246367389396353\n",
      "Step:  70000  loss:  nan  accuracy_train:  0.713915  f1_score_train:  0.17592309691776606  accuracy_test:  0.7126670876281783  f1_score_test:  0.17246367389396353\n",
      "Step:  80000  loss:  nan  accuracy_train:  0.7139  f1_score_train:  0.17591477873846267  accuracy_test:  0.7126404069689882  f1_score_test:  0.17247852551339668\n",
      "Step:  90000  loss:  nan  accuracy_train:  0.713895  f1_score_train:  0.17592309691776606  accuracy_test:  0.7126315134159248  f1_score_test:  0.17247852551339668\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 100000\n",
    "lr = 0.005\n",
    "w = np.zeros(31)\n",
    "%run implementations.py\n",
    "weights_log_reg = logistic_regression(x_train_log_stand, y_train, x_test_log_stand, y_test, w, lr, num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tX_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-537e9d75b8b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tX_test' is not defined"
     ]
    }
   ],
   "source": [
    "tX_test = np.insert(tX_test, 0, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "time_day = datetime.datetime.now().day\n",
    "time_hour = datetime.datetime.now().hour\n",
    "time_min = datetime.datetime.now().minute\n",
    "time_second = datetime.datetime.now().second\n",
    "\n",
    "time = str(time_day)+\"-\"+str(time_hour)+\"-\"+str(time_min)+\"-\"+str(time_second)\n",
    "\n",
    "OUTPUT_PATH = 'submission'+\"_\"+str(time)+\".csv\"\n",
    "print(weights_LS.shape)\n",
    "y_pred = predict_labels(weights_LS, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
